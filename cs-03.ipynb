{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2833de3f-5acb-427d-ae0b-aedda7531687",
   "metadata": {},
   "source": [
    "# Finite Markov Decision Processes "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665c7ce8-f76b-4e84-b136-f14de2fc72e6",
   "metadata": {},
   "source": [
    "![Tangent Plane](img/agent-env.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfd5388-fc7d-480a-9a6b-f6867f8efca8",
   "metadata": {},
   "source": [
    "- The learner and decision maker is called the agent. \n",
    "- The thing it interacts with, comprising everything outside the agent, is called the environment\n",
    "- The environment also gives rise to rewards, special numerical values that the agent seeks to maximize over time through its choice of actions. \n",
    "\n",
    "More specifically, the agent and environment interact at each of a sequence of discrete time steps, t = 0, 1, 2, 3 , … .\n",
    "- At each time step t, the agent receives some representation of the environment’s state, $S_t \\in \\mathcal{S}$, \n",
    "- and on that basis selects an action, $A_t \\in \\mathcal{A}(s)$ \n",
    "- One time step later, in part as a consequence of its action, the agent receives a numerical reward, $R_{t+1} \\in \\mathcal{R} \\subset \\mathbb{R} $ \n",
    "- and finds itself in a new state, $S_{t+1}$\n",
    "\n",
    "The MDP and agent together thereby give rise to a sequence or trajectory that begins like this: \n",
    "\n",
    "## $$ S_0, A_0, R_1, S_1, A_1, R_2, S_2, A_2, R_3, S_3, A_3, ...\\text{             (3.1)} $$\n",
    "\n",
    "In a finite MDP, the sets of states, actions, and rewards ( $\\mathcal{S}$ , $\\mathcal{A}$, and  $\\mathcal{R}$) all have a finite number of elements.\n",
    "\n",
    "In this case, the random variables $R_t$ and $S_t$ have well defined discrete probability distributions dependent only on the preceding state and action. That is, for particular values of these random variables, $s' \\in \\mathcal{S}$ and $r \\in \\mathcal{R}$ , there is a probability of those values occurring at time $t$, given particular values of the preceding state and action:\n",
    "### ($p : \\mathcal{S} \\times \\mathcal{R} \\times \\mathcal{S} \\times \\mathcal{A} \\to [0 , 1]$): \n",
    "# $$ p(s', r  |  s, a) \\dot{=} Pr\\{ S_t = s', R_t = r  |  S_{t - 1} = s, A_{t - 1} = a \\} \\text{             (3.2)}$$\n",
    "for all $s', s \\in \\mathcal{S}, r \\in \\mathcal{R}, a \\in \\mathcal{A}$\n",
    "\n",
    "The function p defines the dynamics of the MDP. \n",
    "\n",
    "# $$ \\sum_{s \\in \\mathcal{S}} \\sum_{r \\in \\mathcal{R}} p(s', r | s, a) = 1 \\text{             (3.3)}$$\n",
    "for all $s \\in \\mathcal{S}, a \\in \\mathcal{A}$\n",
    "\n",
    "The state must include information about all aspects of the past agent–environment interaction that make a difference for the future. If it does, then the state is said to have the *Markov property* "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555eb057-d8ab-435f-9bea-819f81d360e8",
   "metadata": {},
   "source": [
    "### Example\n",
    "![Tangent Plane](img/mdp-diagram.png)\n",
    "\n",
    "Useful way of summarizing the dynamics of a finite MDP is as a transition graph as shown above on the right. \n",
    "\n",
    "There are two kinds of nodes: state nodes and action nodes . \n",
    "\n",
    "There is a state node for each possible state (a large open circle labeled by the name of the state), and an action node for each state–action pair (a small solid circle labeled by the name of the action and connected by a line to the state node). \n",
    "\n",
    "Starting in state $s$ and taking action $a$ moves you along the line from state node $s$ to action node $( s, a )$. \n",
    "\n",
    "Then the environment responds with a transition to the next state’s node via one of the arrows leaving action node $( s, a )$. \n",
    "\n",
    "Each arrow corresponds to a triple $( s, s' , a )$, where $s'$ is the next state, and we label the arrow with the transition probability, $p ( s' | s, a )$, and the expected reward for that transition, $r ( s, a, s' )$. \n",
    "\n",
    "Note that the transition probabilities labeling the arrows leaving an action node always sum to 1. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19eb0848-6388-407b-8f8c-d9f079933470",
   "metadata": {},
   "source": [
    "### From the four-argument dynamics function, p , one can compute anything else one might want to know about the environment, such as the state-transition probabilities \n",
    "\n",
    "### ($p : \\mathcal{S} \\times \\mathcal{S} \\times \\mathcal{A} \\to [0 , 1]$)\n",
    "\n",
    "# $$ p(s'  |  s, a) \\dot{=} Pr\\{ S_t = s'  |  S_{t - 1} = s, A_{t - 1} = a \\} = \\sum_{r \\in \\mathcal{R}} p(s', r | s, a) \\text{             (3.4)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f984462-cc91-4a8c-b064-308a34201690",
   "metadata": {},
   "source": [
    "### We can also compute the expected rewards for state–action pairs as a two-argument function \n",
    "\n",
    "### ($r : \\mathcal{S} \\times \\mathcal{A} \\to \\mathbb{R}$)\n",
    "# $$ r(s, a) \\dot{=} \\mathbb{E} \\left[ R_t  |  S_{t - 1} = s, A_{t - 1} = a \\right] = \\sum_{r \\in \\mathcal{R}} r \\sum_{s' \\in \\mathcal{S}} p(s', r | s, a)\\text{             (3.5)} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23346ba0-418a-42fe-a9e4-6bf3f18e8eff",
   "metadata": {},
   "source": [
    "### and the expected rewards for state–action–next-state triples as a three-argument function \n",
    "\n",
    "### ($r : \\mathcal{S} \\times \\mathcal{A} \\times \\mathcal{S} \\to \\mathbb{R}$)\n",
    "# $$ r(s, a, s') \\dot{=} \\mathbb{E} \\left[ R_t  |  S_{t - 1} = s, A_{t - 1} = a, S_{t} = s' \\right] = \\sum_{r \\in \\mathcal{R}} r \\frac{ p(s', r | s, a) }{ p(s' | s, a) }\\text{             (3.6)} $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94eb61ef-d70c-4c58-a893-65c18a961890",
   "metadata": {},
   "source": [
    "### Reward hypothesis:\n",
    "\n",
    "####  All of what we mean by goals and purposes can be well thought of as the maximization of the expected value of the cumulative sum of a received scalar signal (called reward). \n",
    "\n",
    "## The reward signal is your way of communicating to the robot what you want it to achieve, not how you want it achieved. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6bb3b2d-82d2-4d55-bd49-0c04be936e16",
   "metadata": {},
   "source": [
    "## Returns and Episodes \n",
    "\n",
    "In general, we seek to maximize the expected **return**, where the return, denoted $G_t$, is defined as some specific function of the reward sequence. \n",
    "\n",
    "In the simplest case the return is the sum of the rewards: \n",
    "\n",
    "# $$ G_t = R_t + R_{t + 1} + R_{t + 2} + ... + R_T \\text{             (3.7)}$$\n",
    "\n",
    "where T is a final time step. \n",
    "\n",
    "This approach makes sense in applications in which there is a natural notion of final time step, that is, when the agent–environment interaction breaks naturally into subsequences, which we call **episodes**, such as plays of a game, trips through a maze, or any sort of repeated interaction. \n",
    "\n",
    "Each episode ends in a special state called the terminal state , followed by a reset to a standard starting state or to a sample from a standard distribution of starting states. \n",
    "\n",
    "Tasks with episodes of this kind are called **episodic tasks**\n",
    "\n",
    "In episodic tasks we sometimes need to distinguish the set of all nonterminal states, denoted $\\mathcal{S}$ , from the set of all states plus the terminal state, denoted $\\mathcal{S^+}$.\n",
    "\n",
    "The time of termination, $T$ , is a random variable that normally varies from episode to episode. \n",
    "\n",
    "On the other hand, in many cases the agent–environment interaction does not break naturally into identifiable episodes, but goes on continually without limit. For example, this would be the natural way to formulate an on-going process-control task, or an application to a robot with a long life span. We call these **continuing tasks**. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e148b60-a3db-431b-a1d1-adc9793ff76e",
   "metadata": {},
   "source": [
    "## Discounting\n",
    "\n",
    "The additional concept that we need is that of **discounting**. According to this approach, the agent tries to select actions so that the sum of the discounted rewards it receives over the future is maximized. \n",
    "\n",
    "In particular, it chooses $A_t$ to maximize the expected discounted return: \n",
    "\n",
    "# $$ G_t = R_{t + 1} + \\gamma R_{t + 2} + \\gamma^2 R_{t + 3} + ... = \\sum_{k = 0}^{\\infty} \\gamma^k R_{t + k + 1}\\text{             (3.8)} $$\n",
    "\n",
    "where $\\gamma$ is a parameter, $0 \\leqslant \\gamma \\leqslant 1$, called the ***discount rate*** \n",
    "\n",
    "# $$ G_t = R_{t + 1} + \\gamma G_{t + 1}\\text{             (3.9)} $$\n",
    "\n",
    "\n",
    "### If the reward is a constant +1 , then the return is:\n",
    "\n",
    "# $$ G_t = \\sum_{k = 0}^{\\infty} \\gamma^k = \\frac{1}{1 - \\gamma}\\text{             (3.10)} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2c1aff-f44f-474b-b2e9-6e3f950cd6b8",
   "metadata": {},
   "source": [
    "![Tangent Plane](img/mdp-1.png)\n",
    "\n",
    "### We can write:\n",
    "\n",
    "# $$  G_t \\dot{=} \\sum_{k = t + 1}^{T} \\gamma^{k - t - 1} R_k \\text{             (3.11)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff9d601-4ea3-4709-8e04-471a541bee50",
   "metadata": {},
   "source": [
    "## Policies and Value Functions \n",
    "\n",
    "***Value functions*** - functions of states (or of state–action pairs) that estimate how good it is for the agent to be in a given state \n",
    "\n",
    "***Policy*** is a mapping from states to probabilities of selecting each possible action. \n",
    "\n",
    "If the agent is following policy $\\pi$ at time $t$ , then $\\pi( a | s )$ is the probability that $A_t = a$ if $S_t = s$ . \n",
    "\n",
    "the “ | ” in the middle of $\\pi( a | s )$ merely reminds that it defines a probability distribution over $a \\in \\mathcal{A} ( s )$ for each $s \\in \\mathcal{S}$ . \n",
    "\n",
    "Reinforcement learning methods specify how the agent’s policy is changed as a result of its experience. \n",
    "\n",
    "The ***value function*** of a state s under a policy $\\pi$ , denoted $v_{\\pi} ( s )$, is the expected return when starting in $s$ and following $\\pi$ thereafter. For MDPs, we can define $v_{pi}$ formally by \n",
    "\n",
    "# $$ v_{\\pi}(s) \\dot{=} \\mathbb{E}_{\\pi}(G_t | S_t = s) \\dot{=} \\mathbb{E}_{\\pi}\\left[\\sum_{k = 0}^{\\infty} \\gamma^k R_{t + k + 1} | S_t = s\\right]\\text{             (3.12)} $$\n",
    ", for all $ s \\in \\mathcal{S} $\n",
    "### We call the function $v_{\\pi}$ the state-value function for policy $\\pi$.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe59596-e0af-4ba2-9870-70a0e0d3a473",
   "metadata": {},
   "source": [
    "Similarly, we define the value of taking action $a$ in state $s$ under a policy $\\pi$ , denoted $q_{\\pi}( s, a )$, as the expected return starting from $s$ , taking the action $a$ , and thereafter following policy $\\pi$ : \n",
    "\n",
    "# $$ q_{\\pi}(s, a) \\dot{=} \\mathbb{E}_{\\pi}(G_t | S_t = s, A_t = a) \\dot{=} \\mathbb{E}_{\\pi}\\left[\\sum_{k = 0}^{\\infty} \\gamma^k R_{t + k + 1} | S_t = s, A_t = a\\right] \\text{             (3.13)} $$\n",
    "\n",
    "### We call $q_{\\pi}$ the action-value function for policy $\\pi$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5617d67-2c38-425c-a79a-82235f05ea04",
   "metadata": {},
   "source": [
    "### expectation of $R_{t+1}$ in terms of $\\pi$ and the four-argument function $p$\n",
    "\n",
    "# $$ \\mathbb{E}_{\\pi} [R_{t+1} | S_t = s] = \\sum_{a} \\pi(a | s) \\sum_{s', r} r p (s', r | s, a) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0723b7de-2f2b-482f-916a-fa80c69777e4",
   "metadata": {},
   "source": [
    "The value functions $v_{\\pi}$ and $q_{\\pi}$ can be estimated from experience. For example, if an agent follows policy $\\pi$ and maintains an average, for each state encountered, of the actual returns that have followed that state, then the average will converge to the state’s value, $v_{\\pi} ( s )$, as the number of times that state is encountered approaches infinity. If separate averages are kept for each action taken in each state, then these averages will similarly converge to the action values, $q_{\\pi} ( s, a )$\n",
    "\n",
    "### We call estimation methods of this kind ***Monte Carlo*** methods because they involve averaging over many random samples of actual returns. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf50508-afe5-4835-a969-11c124d18153",
   "metadata": {},
   "source": [
    "![Tangent Plane](img/backup-diagram-1.png)\n",
    "We call diagrams like that above backup diagrams because they diagram relationships that form the basis of the update or backup operations that are at the heart of reinforcement learning methods. These operations transfer value information back to a state (or a state–action pair) from its successor states (or state–action pairs). We use backup diagrams throughout the book to provide graphical summaries of the algorithms we discuss. (Note that, unlike transition graphs, the state nodes of backup diagrams do not necessarily represent distinct states; for example, a state might be its own successor.) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823ce559-fbce-4278-b1e0-0ea6f64e13c4",
   "metadata": {},
   "source": [
    "### For any policy π and any state s , the following consistency condition holds between the value of s and the value of its possible successor states: \n",
    "\n",
    "# $$ v_{\\pi} (s) \\dot{=} \\mathbb{E}_{\\pi}\\left[G_t | S_t = s\\right] $$\n",
    "# $$ = \\mathbb{E}_{\\pi}\\left[R_{t + 1} + \\gamma G_{t + 1} | S_t = s\\right] $$\n",
    "# $$ = \\sum_{a}\\pi(a|s) \\sum_{s'} \\sum_{r} p(s', r | s, a) \\left(r + \\gamma \\mathbb{E}_{\\pi}\\left[G_{t+1} | S_{t+1} = s'\\right]\\right) $$\n",
    "# $$ = \\sum_{a}\\pi(a|s) \\sum_{s', r} p(s', r | s, a) \\left(r + \\gamma v_{\\pi}(s')\\right)\\text{             (3.14)} $$\n",
    "### for all $s \\in \\mathcal{S}$\n",
    "\n",
    "### Equation (3.14) is the Bellman equation for v π . \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f71aa3-c5ee-4a69-a54d-4a1f8e14d40d",
   "metadata": {},
   "source": [
    "![Tangent Plane](img/backup-diagram-2.png)\n",
    "### Bellman equation for action values, that is, for $q_{\\pi}$ \n",
    "\n",
    "# $$ q_{\\pi} (s, a) \\dot{=} \\mathbb{E}_{\\pi} \\left[G_t | S_t = s, A_t = a \\right] $$\n",
    "# $$ = \\mathbb{E}_{\\pi} \\left[R_{t + 1} + \\gamma G_{t + 1} | S_t = s, A_t = a \\right] $$\n",
    "# $$ = \\sum_{s', r} p(s', r | s, a) \\left[r + \\gamma \\sum_{a'} \\pi(a' | s') q_{\\pi}(s', a') \\right] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84c6c24-f2e9-4908-8e96-5303343f0e40",
   "metadata": {},
   "source": [
    "![Tangent Plane](img/backup-diagram-3.png)\n",
    "\n",
    "# $$ v_{\\pi}(s) =\\mathbb{E}_{\\pi}\\left[ q_{\\pi}(s, a) \\right] = \\sum_{a} \\pi(a | s) q_{\\pi}(s, a) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e288788-ea6b-472e-a8c6-097915fe1761",
   "metadata": {},
   "source": [
    "![Tangent Plane](img/backup-diagram-4.png)\n",
    "\n",
    "# $$ q_{\\pi}(s, a) = \\mathbb{E}_{\\pi}\\left[ R_{t + 1} + \\gamma v_{\\pi}(S_{t + 1}) | S_t = s, A_t = a \\right] $$\n",
    "# $$ = \\sum_{s', r} p(s', r | s, a) \\left( r + \\gamma v_{\\pi}(s') \\right) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03df8a30-96d5-4640-ad43-1953700736ee",
   "metadata": {},
   "source": [
    "### Optimal Policies and Optimal Value Functions \n",
    "\n",
    "### $ \\pi \\geqslant \\pi'$ if and only if $v_{\\pi} ( s ) \\geqslant v_{\\pi'} ( s )$ for all $s \\in \\mathcal{S}$\n",
    "\n",
    "There is always at least one policy that is better than or equal to all other policies. This is an ***optimal policy***\n",
    "\n",
    "### Optimal state-value function, denoted $v_{*}$ , is defined as: \n",
    "\n",
    "# $$ v_{*}(s) \\dot{=} \\underset{\\pi}{\\operatorname{max}} v_{\\pi}(s) \\text{             (3.15)} $$\n",
    "for all $s \\in \\mathcal{S}$\n",
    "\n",
    "### Optimal policies also share the same optimal action-value function , denoted $q_{*}$, and defined as:\n",
    "\n",
    "# $$ q_{*}(s, a) = \\underset{\\pi}{\\operatorname{max}} q_{\\pi} (s, a) \\text{             (3.16)}  $$\n",
    "for all $s \\in \\mathcal{S}$ and $a \\in \\mathcal{A}(s)$ \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f268669-e322-4043-96cb-8d74308f8f1c",
   "metadata": {},
   "source": [
    "### We can write $q_{*}$ in terms of $v_{*}$ as follows: \n",
    "\n",
    "# $$ q_{*}(s, a) = \\mathbb{E} \\left[ R_{t + 1} + \\gamma v_{*}(S_{t + 1}) | S_t = s, A_t = a \\right] \\text{             (3.17)}   $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c2a223-00a5-4fa0-add4-1ac83aa94a7e",
   "metadata": {},
   "source": [
    "Because it is the optimal value function, however, $v_{*}$'s consistency condition can be written in a special form without reference to any specific policy. \n",
    "\n",
    "This is the Bellman equation for $v_{*}$ , or the Bellman optimality equation . Intuitively, the Bellman optimality equation expresses the fact that the value of a state under an optimal policy must equal the expected return for the best action from that state.\n",
    "### Bellman optimality equation (forms of it are (3.18), (3.19))\n",
    "# $$ v_{*}(s) = \\underset{a \\in \\mathcal{A}(s)}{\\operatorname{max}} q_{\\pi_{*}}(s, a)  $$\n",
    "# $$ = \\underset{a}{\\operatorname{max}} \\mathbb{E}_{\\pi_{*}} \\left[ G_t | S_t= s, A_t = a \\right] $$\n",
    "# $$ = \\underset{a}{\\operatorname{max}} \\mathbb{E}_{\\pi_{*}} \\left[ R_{t + 1} + \\gamma G_{t + 1} | S_t = s, A_t = a \\right] \\text{ (by (3.9)) } $$\n",
    "# $$ = \\underset{a}{\\operatorname{max}} \\mathbb{E} \\left[ R_{t + 1} + \\gamma v_{*}(S_{t + 1}) | S_t = s, A_t = a \\right] \\text{ (3.18) } $$\n",
    "# $$ = \\underset{a}{\\operatorname{max}} \\sum_{s', r} p(s', r | s, a) \\left( r + \\gamma v_{*}(s') \\right) \\text{ (3.19) } $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b33827-9b28-4105-b6ba-35627db84bee",
   "metadata": {},
   "source": [
    "### The Bellman optimality equation for $q_{*}$ is\n",
    "\n",
    "# $$ q_{*}(s, a) = \\mathbb{E}\\left[ R_{t + 1} + \\gamma \\underset{a'}{\\operatorname{max}} q_{*} (S_{t + 1}, a') | S_t = s, A_t = a \\right] $$\n",
    "# $$ = \\sum_{s', r} p(s', r | s, a) \\left( r + \\gamma \\underset{a'}{\\operatorname{max}} q_{*} (s', a') \\right) \\text{ (3.20) } $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8012c8b8-651a-4bd7-b433-56c15af3ce7d",
   "metadata": {},
   "source": [
    "# $$ v_{*}(s) = \\sum_{a} \\pi_{*}(a | s) q_{*}(s, a) $$\n",
    "# $$ q_{*}(s, a) = \\sum_{s', r} p(s', r | s, a) (r + \\gamma v_{*}(s)) $$\n",
    "# $$ \\pi_{*}(a | s) = \\frac{ \\mathbb{1}\\left\\{ a = \\underset{a'}{\\operatorname{argmax}} q_{*}(a', s) \\right\\} }{ \\sum_{a} \\mathbb{1}\\left\\{ a = \\underset{a'}{\\operatorname{argmax}} q_{*}(a', s) \\right\\} } $$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
