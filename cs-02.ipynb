{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8b32798-2bbd-4025-9355-7591af390e0b",
   "metadata": {},
   "source": [
    "# *Sample-average* method for estimating action values \n",
    "\n",
    "## One natural way to estimate sample averages is by averaging the rewards actually received\n",
    "\n",
    "# $ Q_{t}(a) = \\frac{  \\sum_{i=0}^{i=t-1} R_{i} I_{A_{i} = a} }{  \\sum_{i=0}^{i=t-1} I_{A_{i} = a}  } $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5af9769-3689-4b89-947d-52fb9add9f71",
   "metadata": {},
   "source": [
    "## It is easy to devise incremental formulas for updating averages with small, constant computation required to process each new reward. \n",
    "\n",
    "# $  Q_{n + 1} = Q_{n} + \\frac{1}{n}[  R_{n} - Q_{n}  ]  $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fe6396-8516-4d35-93a0-00b1c46e3aec",
   "metadata": {},
   "source": [
    "# A simple bandit algorithm\n",
    "```\n",
    "Initialize, for a = 1 to k:\n",
    "  Q(a) <- 0\n",
    "  N(a) <- 0\n",
    "Loop forever:  \n",
    "  A <- { argmax_{a} Q(a) with probability 1 - epsilon (breaking ties randomly)\n",
    "  A <- { a random action with probability epsilon\n",
    "  R <- bandit(A)\n",
    "  N(A) <- N(A) + 1\n",
    "  Q(A) <- Q(A) + 1/N(A) * (R-Q(A))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcb4d44-2a21-4c97-bd44-1333f2622792",
   "metadata": {},
   "source": [
    "## Upper-Confidence-Bound (UCB) Action Selection \n",
    "\n",
    "# $A_{t} = \\underset{a}{\\operatorname{argmax}}{ [ Q_{t}(a) + c \\sqrt{\\frac{\\ln{t}}{N_{t}(a)}} ] }$\n",
    "\n",
    "where \n",
    "- $\\ln{t}$ denotes the natural logarithm of t, \n",
    "- $N_{t}(a)$ denotes the number of times that action a has been selected prior to time t ), \n",
    "- and the number c > 0 controls the degree of exploration. \n",
    "\n",
    "If N t ( a ) = 0 , then a is considered to be a maximizing action. \n",
    "\n",
    "The square-root term is a measure of the uncertainty or variance in the estimate of a ’s value. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a69705-ef9b-4113-927d-bda305fb6731",
   "metadata": {},
   "source": [
    "# Gradient Bandit\n",
    "\n",
    "We consider learning a numerical preference for each action a, which we denote $H_t(a)$. \n",
    "\n",
    "# $  Pr\\{A_{t} = a\\} = \\frac{ e^{H_{t}(a)} }{  \\sum_{b=1}^{b=k} e^{H_{t}(b)}  } = \\pi_{t}(a)  $\n",
    "\n",
    "### $\\pi_{t}(a)$ is probability of taking action a at time t\n",
    "\n",
    "Initially all action preferences are the same (e.g., $H_1 ( a ) = 0$ , for all a ) so that all actions have an equal probability of being selected.\n",
    "\n",
    "There is a natural learning algorithm for this setting based on the idea of stochastic gradient ascent. \n",
    "\n",
    "On each step, after selecting action A t and receiving the reward R t , the action preferences are updated by: \n",
    "\n",
    "# $  H_{t + 1} (A_t) = H_{t} (A_t) + \\alpha (R_t - \\bar{R_t})(1-\\pi_{t}(A_{t}))  $ &nbsp;&nbsp;&nbsp;&nbsp;, and\n",
    "# $  H_{t + 1} (a) = H_{t} (a) - \\alpha (R_t - \\bar{R_t}) \\pi_{t}(a)  $ &nbsp;&nbsp;&nbsp;&nbsp; - for all $ a \\neq A_{t} $\n",
    "\n",
    "where \n",
    " - α > 0 is a step-size parameter, \n",
    " - $R_t \\in {\\rm I\\!R}$ is the average of all the rewards up through and including time t, which can be computed incrementally. \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e3f066-e66a-44e3-95bf-344b1cc7c60d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
