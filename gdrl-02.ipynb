{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f597a85c-f7b4-4b24-b96a-42bb9e8587d5",
   "metadata": {},
   "source": [
    "# Chapter 2\n",
    "## Agent-environment interaction cycle\n",
    "### The environment commonly has a well-defined task. The goal of this task is defined through the reward signal. The reward signal can be dense, sparse, or anything in between. When you design environments, reward signals are the way to train your agent the way you want. The more dense, the more supervision the agent will have, and the faster the agent will learn, but the more bias you’ll inject into your agent, and the less likely the agent will come up with unexpected behaviors. The more sparse, the less supervision, and therefore, the higher the chance of new, emerging behaviors, but the longer it’ll take the agent to learn.\n",
    "\n",
    "### The set of the observation (or state), the action, the reward, and the new observation (or new state) is called an experience tuple.\n",
    "\n",
    "### The task the agent is trying to solve may or may not have a natural ending. Tasks that have a natural ending, such as a game, are called episodic tasks. Tasks that don’t, such as learning forward motion, are called continuing tasks. The sequence of time steps from the beginning to the end of an episodic task is called an episode. Agents may take several time steps and episodes to learn to solve a task. The sum of rewards collected in a single episode is called a return.\n",
    "\n",
    "### The agent may be designed to learn mappings from observations to actions called policies. \n",
    "### The agent may be designed to learn mappings from observations to new observations and/or rewards called models. \n",
    "### The agent may be designed to learn mappings from observations (and possibly actions) to reward-to-go estimates (a slice of the return) called value functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3d618f-3ec6-43d9-9204-2c81c3500c3e",
   "metadata": {},
   "source": [
    "## The transition function\n",
    "\n",
    "## $$ p(s'|s, a) = P(S_t = s' | S_{t-1} = s, A_{t-1} = a) $$\n",
    "## $$ \\sum_{s' \\in S} p(s' | s, a) = 1, \\quad \\forall s \\in S, \\quad \\forall a \\in A(s) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d76363-9e93-4508-af27-641199eb633b",
   "metadata": {},
   "source": [
    "## The reward function\n",
    "## $$ r(s, a) = \\mathbb{E}[ R_t | S_{t-1} = s, A_{t-1}=a ] $$\n",
    "## $$ r(s, a, s') = \\mathbb{E}[ R_t | S_{t-1} = s, A_{t-1} = a, S_{t} = s' ], \\quad (R_t \\in \\mathcal{R} \\subset \\mathbb{R} ) $$\n",
    "### where $\\mathcal{R}$ is a set of all rewards, $\\mathbb{R}$ - all real numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e9d6a8-40e0-4c28-b00b-aecfab13cac6",
   "metadata": {},
   "source": [
    "## The discount factor\n",
    "\n",
    "### Return:\n",
    "## $$ G_t = R_{t+1} + R_{t+2} + R_{t+3} + \\ldots + R_T $$\n",
    "### Discounted Return:\n",
    "## $$ G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^{2} R_{t+3} + \\ldots + \\gamma^{T-1} R_T $$\n",
    "### or:\n",
    "## $$ G_t = \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} $$\n",
    "## $$ G_t = R_{t+1}+\\gamma G_{t+1} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb68157a-051f-45c2-bbc6-b867dfb159bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
